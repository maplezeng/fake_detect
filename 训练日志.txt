(base) root@autodl-container-444d118e52-ce51dc92:~/autodl-tmp/fnd-bootstrap# python /root/autodl-tmp/fnd-bootstrap/UAMFD.py -train_dataset Gossipcop_LLM -test_dataset Gossipcop_LLM -batch_size 32 -epochs 100 -val 0 -is_sample_positive 1.0 -duplicate_fake_times 0 -network_arch UAMFDv2 -is_filter 0 -not_on_12 1 -output_file /root/autodl-tmp/save/
Namespace(batch_size=32, checkpoint='', class_num=2, duplicate_fake_times=0, embed_dim=32, epochs=100, finetune=0, get_MLP_score=0, hidden_dim=512, is_filter=0, is_sample_positive=1.0, lambd=1, network_arch='UAMFDv2', not_on_12=1, output_file='/autodl-tmp/save/', sequence_length=25, static=True, test_dataset='Gossipcop_LLM', testing_file='', text_only=False, train_dataset='Gossipcop_LLM', training_file='', val=0, validation_file='', vocab_size=25)
Using amp (Tempt)
loading data
loading checkpoint from 
Filter the dataset? False
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 143988/143988 [00:01<00:00, 75367.80it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 79822.99it/s]
building model
Network UAMFDv2
we are using adaIN
BERT: using /root/autodl-tmp/bert-base-chinese
We are using SRM in Inception
We are using antialias in Inception
{'Total': 284407248, 'Trainable': 284155088}
THE CURRENT MODE FOR FINETUNING:False
Using CosineAnnealingLR+UntunedLinearWarmup
loader size 4499
training model
    32/143988 [..........] - ETA: 2:37:59 - CE_loss: 0.705744 - Image: 0.686955 - Text: 0.761964 - VGG: 0.694358 - aux: 0.485538 - irr_m: 1.000000 - mean_acc: 0.375000/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
143968/143988 [========>.] - ETA: 0s - CE_loss: 0.559554 - Image: 0.696274 - Text: 0.538007 - VGG: 0.693191 - aux: 0.471902 - irr_m: 0.998557 - mean_acc: 0.676648Epoch [1/100],  Loss: 1.2020, Train_Acc: 0.7271,  
end training...
begin evaluate...
thresh: [0.46, 0.462, 0.464, 0.46599999999999997, 0.46799999999999997, 0.47, 0.472, 0.474, 0.476, 0.478, 0.48, 0.482, 0.484, 0.486, 0.488, 0.49, 0.492, 0.494, 0.496, 0.498, 0.5, 0.502, 0.504, 0.506, 0.508, 0.51, 0.512, 0.514, 0.516, 0.518, 0.52, 0.522, 0.524, 0.526, 0.528, 0.53, 0.532, 0.534, 0.536, 0.538]
16000/16000 [==========] - 283s 18ms/step - mix_loss: 2.119038 - image_loss: 0.693184 - text_loss: 0.480680 - vgg_loss: 120.176804
Excel Saved at /root/autodl-tmp/groupshare/mae-main/example/Gossipcop_LLM_experiment.xlsx
Epoch [1/100],  Val_Acc: 0.6441. at thresh 15.0000 (so far 0.6441 in Epoch 1) .
Single Modalities Accuracy: Img 0.5 Text 0.775125 VGG 0.5 SSIM 0.4295625
------Real News -----------
Precision: [0.8465960665658093, 0.845108695652174, 0.8450874020494273, 0.843937575030012, 0.84295542925516, 0.841870160810006, 0.8407605466428996, 0.8403436018957346, 0.8389479905437353, 0.8383957534650546, 0.8378854625550661, 0.8369979478158898, 0.836943793911007, 0.8358644859813084, 0.8358121901428988, 0.8351744186046511, 0.8333816285134743, 0.8329473074696004, 0.8318890814558059, 0.8296402877697842, 0.8281205164992826, 0.8265598168288495, 0.8265714285714286, 0.8263473053892215, 0.8250497017892644, 0.8240294701048456, 0.8222849083215797, 0.8227349465391108, 0.820757363253857, 0.8197820620284996, 0.8189415041782729, 0.8186874304783093, 0.8177025527192009, 0.8165467625899281, 0.8148454746136865, 0.8144472015439758, 0.8141007986780501, 0.8137362637362637, 0.8126027397260274, 0.8110989611809732]
Recall: [0.34975, 0.349875, 0.3505, 0.3515, 0.35225, 0.353375, 0.35375, 0.354625, 0.354875, 0.355375, 0.356625, 0.356875, 0.357375, 0.35775, 0.35825, 0.359125, 0.3595, 0.359625, 0.36, 0.360375, 0.36075, 0.361, 0.361625, 0.36225, 0.363125, 0.3635, 0.364375, 0.3655, 0.36575, 0.36675, 0.3675, 0.368, 0.368375, 0.368875, 0.369125, 0.36925, 0.3695, 0.37025, 0.37075, 0.370875]
Accuracy: [0.34975, 0.349875, 0.3505, 0.3515, 0.35225, 0.353375, 0.35375, 0.354625, 0.354875, 0.355375, 0.356625, 0.356875, 0.357375, 0.35775, 0.35825, 0.359125, 0.3595, 0.359625, 0.36, 0.360375, 0.36075, 0.361, 0.361625, 0.36225, 0.363125, 0.3635, 0.364375, 0.3655, 0.36575, 0.36675, 0.3675, 0.368, 0.368375, 0.368875, 0.369125, 0.36925, 0.3695, 0.37025, 0.37075, 0.370875]
F1: [0.4950022114108801, 0.4948727015558699, 0.49549390351652234, 0.49629368160960113, 0.49687031649475444, 0.49779890825849615, 0.49797642090445193, 0.49876933895921244, 0.4987702037947997, 0.49916600825212887, 0.5003068829460763, 0.5003943563228463, 0.5008759635599159, 0.5010504201680672, 0.5015311925802782, 0.5022727272727273, 0.5023142083660815, 0.502357255107386, 0.5025300994590822, 0.5024836601307189, 0.5025685676969961, 0.5025230555072211, 0.5031304347826087, 0.5036934040149473, 0.5042965020397535, 0.5044669962702749, 0.5049805110437419, 0.5061450579885753, 0.5060095114569823, 0.5067795146385699, 0.5073339085418463, 0.5077612969989651, 0.5079283005860048, 0.5081797830204925, 0.508086717136958, 0.5081276339554486, 0.5082967930530478, 0.50893470790378, 0.5091845493562233, 0.5090066906845085]
------Fake News -----------
Precision: [0.590232374950768, 0.5900851197982345, 0.5902854439362877, 0.590464161667193, 0.5905822864817888, 0.5908084163898117, 0.5907867658698749, 0.5910171102661597, 0.5909162967660114, 0.5910064239828694, 0.5913457721317983, 0.5913098736992612, 0.5914653528289892, 0.5914440203562341, 0.5915997136266009, 0.5917993630573248, 0.5916806120009562, 0.5916626813326957, 0.5916414101132557, 0.5914570858283433, 0.5913703555733121, 0.591236206620822, 0.59144, 0.5916113023293044, 0.5917140796538184, 0.5916927271269344, 0.5917302288237656, 0.5921581230917564, 0.5919581825492561, 0.5921423395861847, 0.5922643029814666, 0.5923895517574976, 0.5923685059696676, 0.5923623445825933, 0.5921945701357466, 0.5921765133758992, 0.592206322257256, 0.5923948220064725, 0.5923886639676114, 0.5922054772322152]
Recall: [0.936625, 0.935875, 0.93575, 0.935, 0.934375, 0.933625, 0.933, 0.932625, 0.931875, 0.9315, 0.931, 0.9305, 0.930375, 0.92975, 0.929625, 0.929125, 0.928125, 0.927875, 0.92725, 0.926, 0.925125, 0.92425, 0.924125, 0.923875, 0.923, 0.922375, 0.92125, 0.92125, 0.920125, 0.919375, 0.91875, 0.9185, 0.917875, 0.917125, 0.916125, 0.915875, 0.915625, 0.91525, 0.9145, 0.913625]
Accuracy: [0.936625, 0.935875, 0.93575, 0.935, 0.934375, 0.933625, 0.933, 0.932625, 0.931875, 0.9315, 0.931, 0.9305, 0.930375, 0.92975, 0.929625, 0.929125, 0.928125, 0.927875, 0.92725, 0.926, 0.925125, 0.92425, 0.924125, 0.923875, 0.923, 0.922375, 0.92125, 0.92125, 0.920125, 0.919375, 0.91875, 0.9185, 0.917875, 0.917125, 0.916125, 0.915875, 0.915625, 0.91525, 0.9145, 0.913625]
F1: [0.7241362647982604, 0.7238012374323279, 0.7239145150372305, 0.7238242694019742, 0.7237256135934551, 0.7236701869973841, 0.723466123873219, 0.7235259891388673, 0.7232246798603027, 0.7231791935562134, 0.723282350084972, 0.7231045704016709, 0.7231830547998445, 0.7229782270606532, 0.7230567303485489, 0.7230544747081711, 0.7226629033042969, 0.7225737369804341, 0.7223682929204401, 0.7218514007308162, 0.7215208384109189, 0.7211547839656687, 0.7212682926829268, 0.7213194749426632, 0.7211289613750672, 0.7209222802989593, 0.7206062087509166, 0.7209234080015652, 0.7204306337166625, 0.7203369080848147, 0.7202351788339051, 0.7202509311899629, 0.7200431457148461, 0.7198077111743354, 0.7193757361601885, 0.7192853286212143, 0.7192302027590947, 0.7192534381139489, 0.7190171990171991, 0.7186117392586766]
---------------------------
end evaluate...
Model saved at /autodl-tmp/save/Gossipcop_LLM/1_710_64.pkl