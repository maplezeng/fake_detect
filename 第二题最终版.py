import pandas as pd
import re
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from gensim import corpora, models
from wordcloud import WordCloud
import pyLDAvis.gensim_models
import pyLDAvis
import numpy as np
import ollama

plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

# -------- 1. æ•°æ®å‡†å¤‡ --------
data = pd.read_csv("1(1).txt", sep='\t')
texts = data['post_text'].dropna().tolist()

# -------- 2. æ•°æ®é¢„å¤„ç† --------
nltk.download('stopwords')
nltk.download('wordnet')
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
def remove_urls(text):
    return re.sub(r'http\S+', '', text)
def preprocess(text):
    text = remove_urls(text)
    text = re.sub(r'[^a-zA-Z]', ' ', text).lower()
    words = text.split()
    return [lemmatizer.lemmatize(w) for w in words if w not in stop_words and len(w) > 2]

processed_texts = [preprocess(t) for t in texts]

# -------- æ‰“å°é¢„å¤„ç†ç»“æœ --------
for i, (original, processed) in enumerate(zip(texts, processed_texts)):
    print(f"\nã€åŸå§‹æ–‡æœ¬ {i+1}ã€‘:\n{original}")
    print(f"ã€é¢„å¤„ç†åã€‘:\n{' '.join(processed)}")


# -------- 3. æ„å»ºè¯è¢‹æ¨¡å‹å¹¶è®­ç»ƒLDA --------
# åˆ›å»ºè¯å…¸å’Œè¯­æ–™åº“
dictionary = corpora.Dictionary(processed_texts)
corpus = [dictionary.doc2bow(text) for text in processed_texts]
# è®­ç»ƒLDAæ¨¡å‹ï¼ˆè®¾ç½®ä¸»é¢˜æ•°ä¸º3ï¼‰
lda_model = models.LdaModel(corpus=corpus,
                            id2word=dictionary,
                            num_topics=3,
                            passes=10,
                            random_state=42)
# æ‰“å°ä¸»é¢˜å…³é”®è¯
print("LDAä¸»é¢˜å…³é”®è¯ï¼š")
for idx, topic in lda_model.print_topics(num_words=10):
    print(f"ä¸»é¢˜ {idx + 1}: {topic}")

# -------- 4. å¯è§†åŒ–åˆ†æ --------
# 4.1 LDAäº¤äº’å›¾
vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)
pyLDAvis.save_html(vis, 'lda_twitter_topics.html')
print("âœ… å·²ä¿å­˜äº¤äº’å¼LDAå¯è§†åŒ–ä¸º lda_twitter_topics.htmlï¼Œè¯·ç”¨æµè§ˆå™¨æ‰“å¼€æŸ¥çœ‹")

# 4.2 è¯äº‘å›¾
for t in range(lda_model.num_topics):
    plt.figure()
    wc = WordCloud(background_color='white').fit_words(dict(lda_model.show_topic(t, 30)))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"ä¸»é¢˜ {t + 1}")
    plt.show()

# 4.3 çƒ­åŠ›å›¾
doc_topic_matrix = [lda_model.get_document_topics(bow) for bow in corpus]
matrix = np.zeros((len(corpus), lda_model.num_topics))

for doc_idx, doc in enumerate(doc_topic_matrix):
    for topic_id, prob in doc:
        matrix[doc_idx][topic_id] = prob

sns.heatmap(matrix, cmap='YlGnBu', xticklabels=[f"ä¸»é¢˜{i + 1}" for i in range(lda_model.num_topics)])
plt.xlabel("ä¸»é¢˜")
plt.ylabel("æ–‡æ¡£ç´¢å¼•")
plt.title("æ–‡æ¡£-ä¸»é¢˜æ¦‚ç‡åˆ†å¸ƒçƒ­åŠ›å›¾")
plt.show()


# -------- 5. ä½¿ç”¨å¤§æ¨¡å‹DeepSeek-R1åšä¸»é¢˜å†…å®¹æ€»ç»“ --------
def analyze_topic_keywords(topic_words, topic_id):
    keywords = ", ".join([w for w, _ in topic_words])
    prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹å…³é”®è¯æ€»ç»“ä¸€ä¸ªTwitterä¸»é¢˜çš„ä¸»è¦å†…å®¹ï¼Œä¸è¶…è¿‡150å­—ï¼š
å…³é”®è¯ï¼š{keywords}"""

    response = ollama.chat(
        model='deepseek-r1:1.5b',
        messages=[{"role": "user", "content": prompt}]
    )
    print(f"\nğŸ§  ä¸»é¢˜ {topic_id + 1} æ™ºèƒ½æ€»ç»“ï¼š\n{response['message']['content']}\n")


for t in range(lda_model.num_topics):
    topic_words = lda_model.show_topic(t, topn=10)
    analyze_topic_keywords(topic_words, t)
